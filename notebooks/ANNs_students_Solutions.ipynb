{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Booston houses dataset from sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "boston_dataset = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 14 attributes in each case of the dataset. They are:\n",
    "\n",
    "1- CRIM - per capita crime rate by town\n",
    "\n",
    "2- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "3- INDUS - proportion of non-retail business acres per town.\n",
    "\n",
    "4- CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "\n",
    "5- NOX - nitric oxides concentration (parts per 10 million)\n",
    "\n",
    "6- RM - average number of rooms per dwelling\n",
    "\n",
    "7- AGE - proportion of owner-occupied units built prior to 1940\n",
    "\n",
    "8- DIS - weighted distances to five Boston employment centres\n",
    "\n",
    "9- RAD - index of accessibility to radial highways\n",
    "\n",
    "10- TAX - full-value property-tax rate per $10,000\n",
    "\n",
    "11- PTRATIO - pupil-teacher ratio by town\n",
    "\n",
    "12- B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "\n",
    "13- LSTAT - % lower status of the population\n",
    "\n",
    "14- MEDV - Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features columns\n",
    "boston_houses = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "x = boston_houses\n",
    "boston_houses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features columns\n",
    "boston_houses.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets column\n",
    "boston_houses['MEDV'] = boston_dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Plot DIS column vs MEDV column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot DIS value against median value\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(boston_houses['DIS'], boston_houses['MEDV'])\n",
    "plt.ylabel('DIS')\n",
    "plt.xlabel('Median Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Split data into train and test sets\n",
    "\n",
    "### Hint: Assign x and y first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and testing sets\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "\n",
    "x = boston_houses[boston_dataset.feature_names]\n",
    "y = boston_houses['MEDV']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Normalize data\n",
    "\n",
    "### Data normalization:\n",
    "    - https://en.wikipedia.org/wiki/Feature_scaling\n",
    "    \n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "x_norm = preprocessing.normalize(x)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_norm, y, test_size=0.3, random_state=0) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 Create an MLP model with sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "modelReg = MLPRegressor(hidden_layer_sizes = 500,\n",
    "                        activation = 'relu',\n",
    "                        solver = 'adam',\n",
    "                        learning_rate_init = 0.003,\n",
    "                        max_iter=2000,\n",
    "                        random_state=0, \n",
    "                        verbose = 'True')\n",
    "\n",
    "modelReg.fit(x_train, y_train)\n",
    "modelReg.predict(x_test)\n",
    "\n",
    "modelReg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
